{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXMfmCcvuzJg"
      },
      "source": [
        "# Assignment: Voter classification using exit poll data\n",
        "\n",
        "*Fraida Fund*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIgOH_WfuzJh"
      },
      "source": [
        "**TODO**: Edit this cell to fill in your NYU Net ID and your name:\n",
        "\n",
        "-   **Net ID**:lz3546\n",
        "-   **Name**:Luxi Zhang"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6LLRqOuuzJh"
      },
      "source": [
        "In this notebook, we will explore the problem of voter classification.\n",
        "\n",
        "Given demographic data about a voter and their opinions on certain key issues, can we predict their vote in the 2020 U.S. presidential election? We will attempt this using a K nearest neighbor classifier.\n",
        "\n",
        "In the first few sections of this notebook, I will show you how to prepare the data and and use a K nearest neighbors classifier for this task, including:\n",
        "\n",
        "-   getting the data and loading it into the workspace.\n",
        "-   preparing the data: dealing with missing data, encoding categorical data in numeric format, and splitting into training and test.\n",
        "\n",
        "In the last few sections of the notebook, you will have to improve the basic model for better performance, using a custom distance metric and using feature weighting. In these sections, you will have specific criteria to satisfy for each task.\n",
        "\n",
        "**However, you should also make sure your overall solution is good!** An excellent solution to this problem will achieve greater than 80% test accuracy. (It is possible to achieve an accuracy greater than 85%.) A great solution will achieve 75% or higher.\n",
        "\n",
        "(A K nearest neighbor model is not necessarily the best model for this data - but the experience of *improving* the K nearest neighbor model will be a useful learning experience for us. In particular, it is a nice demonstration of how plugging some data into an `sklearn` classifier is not the beginning and end of model training!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGSXgOBfuzJi"
      },
      "source": [
        "#### üìù Specific requirements\n",
        "\n",
        "-   For full credit, you should achieve 75% or higher test accuracy overall in this notebook (i.e.¬†when running your solution notebook from beginning to end).\n",
        "-   Also, you should achieve no less than 70% test accuracy on any survey version."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDpi2oLauzJi"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2nd67pGuzJi"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import ShuffleSplit, KFold, train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "np.set_printoptions(suppress=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQDGfD4RuzJj"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzcPYzuSuzJj"
      },
      "source": [
        "The data for this notebook comes from the [U.S. National Election Day Exit Polls](https://ropercenter.cornell.edu/exit-polls/us-national-election-day-exit-polls).\n",
        "\n",
        "Here‚Äôs a brief description of how election day exit polls work.\n",
        "\n",
        "Exit polls are conducted by Edison Research on behalf of a consortium of media organizations.\n",
        "\n",
        "First, the member organizations decide what races to cover, what sample size they want, what questions should be asks, and other details. Then, sample precincts are selected, and local interviewers are hired and trained. Then, at those precincts, the local interviewer approaches a subset of voters as they exit the polls (for example, every third voter, or every fifth voter, depending on the required sample size).\n",
        "\n",
        "When a voter is approached, they are asked if they are willing to fill out a questionnaire. Typically about 40-50% agree. (For those that decline, the interviewer visually estimates their age, race, and gender, and notes this information, so that the response rate by demographic is known and responses can be weighted accordingly in order to be more representative of the population.)\n",
        "\n",
        "Voters that agree to participate are then given an form with 15-20 questions. They fill in the form (anonymously), fold it, and put it in a small ballot box.\n",
        "\n",
        "Several times during the day, the interviewers will stop, take the questionnaires, compile the results, and call them in to the Edison Research phone center. The results are reported immediately to the media organizations that are consortium members.\n",
        "\n",
        "In addition to the poll of in-person voters, absentee and early voters (who are not at the polls on Election Day) are surveyed by telephone."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SF9tPgufuzJj"
      },
      "source": [
        "### Download the data and documentation\n",
        "\n",
        "The exit poll data is not freely available on the web, but is available to those with institutional membership. You will be able to use your NYU email address to create an account with which you can download the exit poll data.\n",
        "\n",
        "To get the data:\n",
        "\n",
        "1.  Visit [the Roper Center website via NYU Libraries link](https://persistent.library.nyu.edu/arch/NYU02495). Click on the user icon in the top right of the page, and choose ‚ÄúLog in‚Äù.\n",
        "2.  For ‚ÄúYour Affiliation‚Äù, choose ‚ÄúNew York University‚Äù.\n",
        "3.  Then, click on the small red text ‚ÄúRegister‚Äù below the password input field. The email and password fields will be replaced by a new email field with two parts.\n",
        "4.  Enter your NYU email address in the email field, and then click the red ‚ÄúRegister‚Äù button.\n",
        "5.  You will get an email at your NYU email address with the subject ‚ÄúRoper iPoll Account Registration‚Äù. Open the email and click ‚ÄúConfirm Account‚Äù to create a password and finish your account registration.\n",
        "6.  Once you have completed your account registration, log in to Roper iPoll by clicking the user icon in the top right of the page, choosing ‚ÄúLog in‚Äù, and entering your NYU email address and password.\n",
        "7.  Then, open the Study Record for the [2020 National Election Day Exit Poll](https://ropercenter.cornell.edu/ipoll/study/31119913).\n",
        "8.  Click on the ‚ÄúDownloads‚Äù tab, and then click on the CSV data file in the ‚ÄúDatasets‚Äù section of this tab. Press ‚ÄúAccept‚Äù to accept the terms and conditions. Find the file `31119913_National2020.csv` in your browser‚Äôs default download location.\n",
        "9.  After you download the CSV file, also download the PDF `31119913_National2020.pdf` from the ‚ÄúStudy Documentation‚Äù section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeQAMKIsuzJj"
      },
      "source": [
        "### Upload into Colab filesystem\n",
        "\n",
        "To get the data into Colab, run the following cell. Upload the CSV file you just downloaded (`31119913_National2020.csv`) to your Colab workspace. Wait until the uploaded has **completely** finished - it may take a while, depending on the quality of your network connection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hzk_60pEuzJj"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  from google.colab import files\n",
        "\n",
        "  uploaded = files.upload()\n",
        "\n",
        "  for fn in uploaded.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "        name=fn, length=len(uploaded[fn])))\n",
        "\n",
        "except:\n",
        "  pass # not running in Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd5-FdxhuzJj"
      },
      "source": [
        "### Load data with pandas\n",
        "\n",
        "Now, use the `read_csv` function in `pandas` to read in the file.\n",
        "\n",
        "Also use `head` to view the first few rows of data and make sure that everything is read in correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CCOSfXVuzJj"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('31119913_National2020.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvaS_NJnuzJj"
      },
      "source": [
        "## Prepare data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "legkV8MGuzJj"
      },
      "source": [
        "Survey data can be tricky to work with, because surveys often ‚Äúbranch‚Äù; the questions that are asked depends on a respondent‚Äôs answers to other questions.\n",
        "\n",
        "In this case, different respondents fill out different versions of the survey. Review pages 17, 18, 19, 20 of the ‚ÄúStudy Documentation‚Äù PDF file you downloaded earlier, which shows the different questionnaire versions used for the 2020 exit polls.\n",
        "\n",
        "Note that in a red box next to each question, you can see the name of the variable (column name) that the respondent‚Äôs answer will be stored in."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1fAsf7RuzJj"
      },
      "source": [
        "<figure>\n",
        "<img src=\"https://raw.githubusercontent.com/ffund/ml-notebooks/master/notebooks/images/exit-poll-survey-versions-2020.png\" alt=\"Exit poll versions\" />\n",
        "<figcaption aria-hidden=\"true\">Exit poll versions</figcaption>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVokFSvquzJj"
      },
      "source": [
        "This cell will tell us how many respondents answered each version of the survey:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xt-BgsDluzJk"
      },
      "outputs": [],
      "source": [
        "df['version'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ioSO9RuuzJk"
      },
      "source": [
        "Because each respondent answers different questions, for each row in the data, only some of the columns - the columns corresponding to questions included in that version of the survey - have data. Our classifier will need to handle that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VISpCGJjuzJk"
      },
      "source": [
        "You may also notice that the data is *categorical*, not *numeric* - for each question, users choose their response from a finite set of possible answers. We will need to convert this type of data into something that our classifier can work with."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZx1LjCRuzJk"
      },
      "source": [
        "### Label missing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9bbTZ47uzJk"
      },
      "source": [
        "Since each respondent only saw a subset of questions, we expect to see missing values in each column.\n",
        "\n",
        "However, if we look at the **count** of values in each column, we see that there are no missing values - every column has the full count!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aajRM40UuzJk"
      },
      "outputs": [],
      "source": [
        "df.describe(include='all')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgLhlQR6uzJk"
      },
      "source": [
        "This is because missing values are recorded as a single space, and not with a NaN.\n",
        "\n",
        "Let‚Äôs change that:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYBcIFJuuzJk"
      },
      "outputs": [],
      "source": [
        "df.replace(\" \", float(\"NaN\"), inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQ9rcXmiuzJk"
      },
      "source": [
        "Now we can see an accurate count of the number of responses in each column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCdNqCUJuzJk"
      },
      "outputs": [],
      "source": [
        "df.describe(include='all')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKZL6Zf_uzJk"
      },
      "source": [
        "Notice that *every* row has some missing data! If we drop the rows with missing values, we‚Äôre left with an empty data frame (0 rows):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lD0teMT0uzJk"
      },
      "outputs": [],
      "source": [
        "df.dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeVu13s3uzJk"
      },
      "source": [
        "Instead, we‚Äôll have to make sure that the classifier we use is able to work with partial data. One nice benefit of K nearest neighbors is that it can work well with data that has missing values, as long as we use a distance metric that behaves reasonably under these conditions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4B7zBfIuzJk"
      },
      "source": [
        "### Encode target variable as a binary variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUH6_zR8uzJk"
      },
      "source": [
        "Our goal is to classify voters based on their vote in the 2020 presidential election, i.e.¬†the value of the `pres` column. We will restrict our attention to the candidates from the two major parties, so we will throw out the rows representing voters who chose other candidates:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUd-weZ6uzJk"
      },
      "outputs": [],
      "source": [
        "df['pres'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-shBUKwuzJk"
      },
      "outputs": [],
      "source": [
        "df = df.loc[df['pres'].isin(['Joe Biden', 'Donald Trump'])]\n",
        "df.reset_index(inplace=True, drop=True)\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWV9BPJNuzJk"
      },
      "outputs": [],
      "source": [
        "df['pres'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUOU-fHDuzJk"
      },
      "source": [
        "Now, we will transform the string value into a binary variable, and save the result in `y`. We will build a binary classifier that predicts `1` if it thinks a sample is Trump voter, and `0` if it thinks a sample is a Biden voter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDYBAUapuzJk"
      },
      "outputs": [],
      "source": [
        "y = df['pres'].map({'Donald Trump': 1, 'Joe Biden': 0})\n",
        "y.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjjT81g-uzJk"
      },
      "source": [
        "### Encode ordinal features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8s7caKeuzJk"
      },
      "source": [
        "Next, we need to encode our features. All of the features are represented as strings, but we will have to transform them into something over which we can compute a meaningful distance measure.\n",
        "\n",
        "Columns that have a **logical order** should be encoded using ordinal encoding, so that the distance metric will be meaningful.\n",
        "\n",
        "For example, consider the `age` column, in which users select an option from the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSEN4vO8uzJn"
      },
      "outputs": [],
      "source": [
        "df['age'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lq8VWfmyuzJo"
      },
      "source": [
        "What if we would transform the `age` column using four binary columns: `age_18-29`, `age_30-44`, `age_45-64`, `age_65+`, with a 0 or a 1 in each column to indicate the respondent‚Äôs age?\n",
        "\n",
        "If we did this, we would lose meaningful information about the distance between ages; a respondent whose age is 18-29 would have the same distance to one whose age is 45-65 as to one whose age is 65+. Logically, we expect that a respondent whose age is 18-29 is most similar to the other 18-29 respondents, less similar to the 30-44 respondents, even less similar to the 45-65 respondents, and least similar to the 65+ respondents.\n",
        "\n",
        "To realize this, we will use **ordinal encoding**, which will represent `age` in a single column with *ordered* integer values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6caI1J6JuzJo"
      },
      "source": [
        "First, we define a dictionary that maps each possible value to an integer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6poloTnuzJo"
      },
      "outputs": [],
      "source": [
        "mapping_dict_age = {'18-29': 1,\n",
        "                 '30-44': 2,\n",
        "                 '45-64': 3,\n",
        "                 '65+': 4}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slfZyjqDuzJo"
      },
      "source": [
        "Then we can create a new data frame, `df_enc_ord`, by calling `map` on the original `df['age']` and passing this mapping dictionary. We will also specify that the index should be the same as the original data frame:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WQ7eobDuzJo"
      },
      "outputs": [],
      "source": [
        "df_enc_ord = pd.DataFrame( {'age': df['age'].map( mapping_dict_age) },\n",
        "    index = df.index\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toOO66DpuzJo"
      },
      "source": [
        "We can extend this approach to encode more than one ordinal feature. For example, let us consider the column `educ18`, which includes the respondent‚Äôs answer to the question:\n",
        "\n",
        "> Which best describes your education? You have:\n",
        ">\n",
        "> 1.  Never attended college\n",
        "> 2.  Attended college but received no degree\n",
        "> 3.  Associate‚Äôs degree (AA or AS)\n",
        "> 4.  Bachelor‚Äôs degree (BA or BS)\n",
        "> 5.  An advanced degree after a bachelor‚Äôs degree (such as JD, MA, MBA, MD, PhD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FBZ4Og_uzJo"
      },
      "outputs": [],
      "source": [
        "df['educ18'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mb5hiSRkuzJo"
      },
      "source": [
        "We can map both `age` and `educ18` to ordinal-encoded columns in a new data frame:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNIVu6TquzJo"
      },
      "outputs": [],
      "source": [
        "mapping_dict_age = {'18-29': 1,\n",
        "                 '30-44': 2,\n",
        "                 '45-64': 3,\n",
        "                 '65+': 4}\n",
        "mapping_dict_educ18 =  {\"Never attended college\": 1,\n",
        "                   \"Attended college but received no degree\": 2,\n",
        "                   \"Associate's degree (AA or AS)\": 3,\n",
        "                   \"Bachelor's degree (BA or BS)\": 4,\n",
        "                   \"An advanced degree after a bachelor's degree (such as JD, MA, MBA, MD, PhD)\": 5}\n",
        "df_enc_ord = pd.DataFrame( {\n",
        "    'age': df['age'].map( mapping_dict_age) ,\n",
        "    'educ18': df['educ18'].map( mapping_dict_educ18)\n",
        "    },\n",
        "    index = df.index\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWTcEOLHuzJo"
      },
      "source": [
        "Note that the order matters - the ‚ÄúNever attended college‚Äù answer should have the smallest value, followed by ‚ÄúAttended college but received no degree‚Äù, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qsrnGEAuzJo"
      },
      "source": [
        "Also note that missing values are still treated as missing (not mapped to some value) - this is going to be important, since we are going to design a distance metric that treats missing values sensibly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daKYf17GuzJo"
      },
      "outputs": [],
      "source": [
        "df_enc_ord.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeTWPvL8uzJo"
      },
      "source": [
        "There‚Äôs one more important step before we can use our ordinal-encoded values with KNN.\n",
        "\n",
        "Note that the values in the encoded columns range from 1 to the number of categories. For K nearest neighbors, the ‚Äúimportance‚Äù of each feature in determining the class label would be proportional to its scale (because the value of the feature is used directly in the distance metric). If we leave it as is, any feature with a larger range of possible values will be considered more ‚Äúimportant!‚Äù, i.e.¬†would count more in the distance metric.\n",
        "\n",
        "So, we will re-scale our encoded features to the unit interval. We can do this with the `MinMaxScaler` in `sklearn`.\n",
        "\n",
        "(Note: in general, you‚Äôd ‚Äúfit‚Äù scalers etc. on only the training data, not the test data! In this case, however, the min and max in the training data is just due to our encoding, and will definitely be the same as the test data, so it doesn‚Äôt really matter.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BGSQIfTuzJo"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()\n",
        "\n",
        "# first scale in numpy format, then convert back to pandas df\n",
        "df_scaled = scaler.fit_transform(df_enc_ord)\n",
        "df_enc_ord = pd.DataFrame(df_scaled, columns=df_enc_ord.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cZCf_COuzJo"
      },
      "outputs": [],
      "source": [
        "df_enc_ord.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUV53N6-uzJo"
      },
      "source": [
        "Later, you‚Äôll design a model with more ordinal features. For this initial demo, though, we‚Äôll stick to just those two - age and education - and continue to the next step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdgYzaYjuzJo"
      },
      "source": [
        "### Encode categorical features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RspAdoauzJo"
      },
      "source": [
        "In the previous section, we encoded features that have a logical ordering.\n",
        "\n",
        "Other categorical features, such as `qraceai` (race), have no logical ordering. It would be wrong to assign an ordered mapping to these features. These should be encoded using **one-hot encoding**, which will create a new column for each unique value, and then put a 1 or 0 in each column to indicate the respondent‚Äôs answer.\n",
        "\n",
        "(Note: for features that have two possible values - binary features - either categorical encoding or one-hot encoding would be valid!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joeAdCKZuzJo"
      },
      "outputs": [],
      "source": [
        "df['qraceai'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tiGpq80uzJo"
      },
      "source": [
        "We can one-hot encode this column using the `get_dummies` function in `pandas`. (If we were working with data in a `numpy` array, we might choose to use the [`OneHotEncoder`](https://scikit-learn.org/dev/modules/generated/sklearn.preprocessing.OneHotEncoder.html) in `sklearn` instead, but since we have a `pandas` data frame we will stick to the `pandas` function.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDfXiDhOuzJo"
      },
      "outputs": [],
      "source": [
        "df_enc_oh = pd.get_dummies(df['qraceai'], prefix='race', dtype=np.int32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYQ33QF4uzJo"
      },
      "outputs": [],
      "source": [
        "df_enc_oh.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3piwj59vuzJp"
      },
      "source": [
        "Note that we added a `race` prefix to each column name - this prevents overlap between columns, e.g.¬†if we also encoded another feature where ‚ÄúOther‚Äù was a possible answer. And, it helps us relate the new columns back to the original survey question that they answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVC_Wf2VuzJp"
      },
      "source": [
        "For this survey data, we want to preserve information about missing values - if a sample did not have a value for the `qraceai` feature, we want it to have a NaN in all `race` columns. We can assign NaN to those rows as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prDpiVSkuzJp"
      },
      "outputs": [],
      "source": [
        "df_enc_oh.loc[df['qraceai'].isnull(), df_enc_oh.columns.str.startswith(\"race_\")] = float(\"NaN\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tz6uny9TuzJp"
      },
      "source": [
        "Now, for respondents where this feature is not available, we have a NaN in all `RACE` columns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbczlUtzuzJp"
      },
      "outputs": [],
      "source": [
        "df_enc_oh.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcJA2q2DuzJp"
      },
      "source": [
        "### Stack columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uoj2CGLquzJp"
      },
      "source": [
        "Now, we‚Äôll prepare our feature data, by column-wise concatenating the ordinal-encoded feature columns and the one-hot-encoded feature columns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kZAVIaguzJp"
      },
      "outputs": [],
      "source": [
        "X = pd.concat([df_enc_oh, df_enc_ord], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htt_fs5KuzJp"
      },
      "source": [
        "### Get training and test indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoAvE3AbuzJp"
      },
      "source": [
        "We‚Äôll be working with many different subsets of this dataset, including different columns.\n",
        "\n",
        "So instead of splitting up the data into training, validation, and test sets, we‚Äôll get an array of training indices and an array of test indices. Then, we can use these arrays throughout this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63Y-W8Z7uzJp"
      },
      "outputs": [],
      "source": [
        "idx_tr, idx_ts = train_test_split(np.arange(0, df.shape[0]), test_size = 0.3, random_state = 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zh1Td6vuuzJp"
      },
      "source": [
        "I specified the state of the random number generator for repeatability, so that every time we run this notebook we‚Äôll have the same split. This makes it easier to discuss specific examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JxP4sG5uzJp"
      },
      "source": [
        "Now, we can use the `pandas` function `.iloc` to get the training and test parts of the data set for any column.\n",
        "\n",
        "For example, if we want the training subset of `y`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5NhnkcXuzJp"
      },
      "outputs": [],
      "source": [
        "y.iloc[idx_tr]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZqwQOVsuzJp"
      },
      "source": [
        "or the test subset of `y`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBHpFRCcuzJp"
      },
      "outputs": [],
      "source": [
        "y.iloc[idx_ts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxYTslCDuzJp"
      },
      "source": [
        "Here are the summary statistics for the training data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrX-gx6DuzJp"
      },
      "outputs": [],
      "source": [
        "X.iloc[idx_tr].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FGNo3fpuzJp"
      },
      "source": [
        "## Train a k nearest neighbors classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okkSBnmDuzJp"
      },
      "source": [
        "Now that we have a target variable, a few features, and training and test indices, let‚Äôs see what happens if we try to train a K nearest neighbors classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_cNG18juzJp"
      },
      "source": [
        "### Baseline: ‚Äúprediction by mode‚Äù"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NX0qaVv1uzJp"
      },
      "source": [
        "As a baseline against which to judge the performance of our classifier, let‚Äôs find out the accuracy (on the test set) of a classifier that gives the majority class label (0) to all samples in our test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skqlqMkLuzJp"
      },
      "outputs": [],
      "source": [
        "y_pred_baseline = np.repeat(0, len(y.iloc[idx_ts]))\n",
        "accuracy_score(y.iloc[idx_ts], y_pred_baseline)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4aY_znLuzJp"
      },
      "source": [
        "A classifier trained on the data should do *at least* as well as the one that predicts the majority class label. Hopefully, we‚Äôll be able to do much better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVptVI28uzJp"
      },
      "source": [
        "### `KNeighborsClassifier` does not support data with NaNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWh1qo3IuzJq"
      },
      "source": [
        "We‚Äôve previously seen the `sklearn` implementation of a `KNeighborsClassifier`. However, that won‚Äôt work for this problem. If we try to train a `KNeighborsClassifier` on our data using the default settings, it will fail with the error message\n",
        "\n",
        "    ValueError: Input X contains NaN.\n",
        "\n",
        "See for yourself:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKokY9v9uzJq"
      },
      "outputs": [],
      "source": [
        "clf = KNeighborsClassifier(n_neighbors=3)\n",
        "clf.fit(X.iloc[idx_tr], y.iloc[idx_tr])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fv2U6F64uzJq"
      },
      "source": [
        "This is because we have many missing values in our data. And, as we explained previously, dropping rows with missing values is not a good option for this example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENsjcxqsuzJq"
      },
      "source": [
        "Although we cannot use the `sklearn` implementation of a `KNeighborsClassifier`, we can write our own. We need a few things:\n",
        "\n",
        "-   a function that implements a distance metric\n",
        "-   a function that accepts a distance matrix and returns the indices of the K smallest values for each row\n",
        "-   a function that returns the majority vote of the training samples represented by those indices\n",
        "\n",
        "and we have to be prepared to address complications at each stage!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US6aYnVTuzJq"
      },
      "source": [
        "Note: as an alternative to writing our own implementation of KNN, we could work around the issue of missing values by pre-computing a distance matrix and passing it to the `sklearn` implementation of `KNeighborsClassifer`, but that option has other problems we haven‚Äôt discussed yet. We‚Äôll jump straight to writing our own implementation, since that will give us more flexibility to solve those problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHENgWj4uzJq"
      },
      "source": [
        "### Distance metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7DK5iabuzJq"
      },
      "source": [
        "Let‚Äôs start with the distance metric. Suppose we use an L1 distance computed over the features that are non-NaN for both samples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnoQ6D5vuzJq"
      },
      "outputs": [],
      "source": [
        "def custom_distance(a, b):\n",
        "  d = np.zeros(shape=(a.shape[0], b.shape[0]))\n",
        "  for i, a_row in enumerate(a):\n",
        "    dif = np.abs(np.subtract(a_row,b))    # element-wise absolute difference\n",
        "    # dif will have NaN for each element where either a or b is NaN\n",
        "    l1 = np.nansum(dif, axis=1)  # sum of differences, treating NaN as 0\n",
        "    d[i] = l1\n",
        "  return d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1BV5afEuzJq"
      },
      "source": [
        "The function above expects an array for the first argument with shape `(num_rows_a, d)` and an array for the second argument with shape `(num_rows_b, d)`. It returns an array with shape `(num_rows_a, num_rows_b)`. An entry in row $i$, column $j$ is the distance between row $i$ of array `a` and row `j` of array `b`.\n",
        "\n",
        "To see how to this function is used, let's consider an example with a small number of test samples and training samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kQNuJx9uzJq"
      },
      "source": [
        "Suppose we had this set of data `a` (sampling some specific examples from the real data) for which we want to make predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNDXmI6kuzJq"
      },
      "outputs": [],
      "source": [
        "a_idx = np.array([2881,  7911])\n",
        "a = X.iloc[a_idx]\n",
        "a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9tgjGaduzJq"
      },
      "source": [
        "and this set of training data `b`, which we have stored as our model ‚Äútraining‚Äù step:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2FtnvPsuzJq"
      },
      "outputs": [],
      "source": [
        "b_idx = np.array([9665, 924, 6892, 10632, 11989, 5613, 7035, 7062, 928, 14113])\n",
        "b = X.iloc[b_idx]\n",
        "b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-g9zhQ8duzJq"
      },
      "source": [
        "We need to compute the distance from each sample in the new data `a`, to each sample in the training data `b`.\n",
        "\n",
        "We will set up a *distance matrix* in which to store the results. In the distance matrix, an entry in row $i$, column $j$ represents the distance between row $i$ of the new data and row $j$ of the training set.\n",
        "\n",
        "So the distance matrix should have as many rows as there are test samples, and as many columns as there are training samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCrtVRgFuzJq"
      },
      "outputs": [],
      "source": [
        "distances_custom = custom_distance(X.iloc[a_idx].to_numpy(), X.iloc[b_idx].to_numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZSiT2FOuzJq"
      },
      "source": [
        "Let‚Äôs look at those distances now:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJGkRjZMuzJq"
      },
      "outputs": [],
      "source": [
        "np.set_printoptions(precision=2) # show at most 2 decimal places\n",
        "print(distances_custom)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TlK8U4tuzJq"
      },
      "source": [
        "### Find most common class of k nearest neighbors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dqSEbt7uzJq"
      },
      "source": [
        "Now that we have a distance matrix, for each test sample, we can:\n",
        "\n",
        "-   get an array of indices from the *distance matrix*, sorted in order of increasing distance\n",
        "-   get the list of the K nearest neighbors as the first K elements from that list,\n",
        "-   from those entries - which are indices with respect to the distance matrix - get the corresponding indices in `X` and `y`,\n",
        "-   and then predict the class of the test sample as the most common value of `y` among the nearest neighbors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Iw1L2RNuzJq"
      },
      "outputs": [],
      "source": [
        "k = 3\n",
        "# array of indices sorted in order of increasing distance\n",
        "distances_sorted = np.argsort(distances_custom, axis=1)\n",
        "# first k elements in that list = indices of k nearest neighbors\n",
        "nn_lists = distances_sorted[:, :k]\n",
        "# map indices in distance matrix back to indices in `X` and `y`\n",
        "nn_lists_idx = b_idx[nn_lists]\n",
        "# for each test sample, get the mode of `y` values for the nearest neighbors\n",
        "y_pred =  [y.iloc[nn].mode()[0] for nn in nn_lists_idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8z1rfHYVuzJq"
      },
      "source": [
        "### Example: one test sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRvnGiXDuzJq"
      },
      "source": [
        "For example, this was the first ‚Äúnew‚Äù sample:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEAyP2dvuzJq"
      },
      "outputs": [],
      "source": [
        "X.iloc[[2881]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Lsy5C52uzJq"
      },
      "source": [
        "Here is its distance to each of the training samples in our ‚Äúmini‚Äù training set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRmIrEqquzJr"
      },
      "outputs": [],
      "source": [
        "distances_custom[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVIYWO4auzJr"
      },
      "source": [
        "and here‚Äôs the sorted list of indices from that distance matrix - i.e.¬†the index of the training sample with the smallest distance, the index of the training sample with the second-smallest distance, and so on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GteUE7zAuzJr"
      },
      "outputs": [],
      "source": [
        "distances_sorted[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MAHy-fCuzJr"
      },
      "source": [
        "The indices (in the ‚Äúmini‚Äù training sample) of the 3 nearest neighbors to this test sample are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3B5dD3VDuzJr"
      },
      "outputs": [],
      "source": [
        "nn_lists[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiCg7F8fuzJr"
      },
      "source": [
        "which corresponds to the following sample indices in the complete data `X`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9cOcGN1uzJr"
      },
      "outputs": [],
      "source": [
        "nn_lists_idx[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmsC8Sz5uzJr"
      },
      "source": [
        "So, its closest neighbors in the ‚Äúmini‚Äù training set are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38oF-7LFuzJr"
      },
      "outputs": [],
      "source": [
        "X.iloc[nn_lists_idx[0]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1b9qf0DuzJr"
      },
      "source": [
        "and their corresponding values in `y` are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mT7BOew4uzJr"
      },
      "outputs": [],
      "source": [
        "y.iloc[nn_lists_idx[0]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZFMpYepuzJr"
      },
      "source": [
        "and so the predicted label for the first test sample would be:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcyyIsmAuzJr"
      },
      "outputs": [],
      "source": [
        "y.iloc[nn_lists_idx[0]].mode().values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvJjakD2uzJr"
      },
      "source": [
        "### Example: entire test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L98A4JD6uzJr"
      },
      "source": [
        "Now that we understand how our custom distance function works, let‚Äôs compute the distance between every *test* sample and every *training* sample.\n",
        "\n",
        "We‚Äôll store the results in `distances_custom`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSEThcEOuzJr"
      },
      "outputs": [],
      "source": [
        "distances_custom = custom_distance(X.iloc[idx_ts].to_numpy(), X.iloc[idx_tr].to_numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soJYYjCNuzJr"
      },
      "source": [
        "Then, we can compute the K nearest neighbors using those distances:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hBlGYuHuzJr"
      },
      "outputs": [],
      "source": [
        "k = 3\n",
        "\n",
        "# get nn indices in distance matrix\n",
        "distances_sorted = np.argsort(distances_custom, axis=1)\n",
        "nn_lists = distances_sorted[:, :k]\n",
        "\n",
        "# map indices in distance matrix back to indices in `X` and `y`\n",
        "nn_lists_idx = idx_tr[nn_lists]\n",
        "\n",
        "# predict using mode of nns\n",
        "y_pred =  [y.iloc[nn].mode()[0] for nn in nn_lists_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-qBECJzuzJr"
      },
      "outputs": [],
      "source": [
        "accuracy_score(y.iloc[idx_ts], y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9RO0EDquzJr"
      },
      "source": [
        "That is‚Ä¶ not great."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6nfzrjxuzJr"
      },
      "source": [
        "### Problems with our simple classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfjYHhomuzJr"
      },
      "source": [
        "Using similar ‚Äútoy‚Äù examples, we will illustrate some basic problems with our classifier, and explain some of the reasons for its poor performance:\n",
        "\n",
        "-   the distance metric does not tell us how *similar* two samples are, when there are samples with missing values,\n",
        "-   and the way that ties are handled - when there are multiple samples in the training set with the same distance - is not ideal.\n",
        "\n",
        "We‚Äôll discuss both of these, but we‚Äôll only fix the second one in this section. Part of *your* assignment will be to address the issue with the custom distance metric in your solution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HyAeHl0uzJr"
      },
      "source": [
        "First, let‚Äôs talk about ties! Consider the following ‚Äúnew‚Äù sample -"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQjGlkA7uzJr"
      },
      "outputs": [],
      "source": [
        "a_idx = np.array([4553])\n",
        "X.iloc[a_idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZ4EumI8uzJr"
      },
      "source": [
        "and training samples -"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZcmuQM-uzJr"
      },
      "outputs": [],
      "source": [
        "b_idx = np.array([13648, 13896, 13916, 13967, 7727, 8073, 8101, 8241])\n",
        "X.iloc[b_idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YS2Tkm6ZuzJr"
      },
      "source": [
        "Notice that these samples are identical with respect to the feature data! but they are not identical with respect to the target variable -"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiwZRRvWuzJr"
      },
      "outputs": [],
      "source": [
        "y.iloc[b_idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MINVqG01uzJs"
      },
      "source": [
        "However, when we train a nearest neighbors classifier on this data -"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxznbOzCuzJs"
      },
      "outputs": [],
      "source": [
        "k = 3\n",
        "\n",
        "# compute distances\n",
        "distances_custom  = custom_distance(X.iloc[a_idx].to_numpy(), X.iloc[b_idx].to_numpy())\n",
        "\n",
        "# get nn indices in distance matrix\n",
        "distances_sorted = np.argsort(distances_custom, axis=1)\n",
        "nn_lists = distances_sorted[:, :k]\n",
        "\n",
        "# map indices in distance matrix back to indices in `X` and `y`\n",
        "nn_lists_idx = b_idx[nn_lists]\n",
        "\n",
        "# predict using mode of nns\n",
        "y_pred =  [y.iloc[nn].mode()[0] for nn in nn_lists_idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehkrpnj2uzJs"
      },
      "source": [
        "we see that although there are eight training samples with *equal* distance to the test sample, the nearest neighbor classifier just takes the first K of them as nearest neighbors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SkvPeSPuzJs"
      },
      "outputs": [],
      "source": [
        "X.iloc[nn_lists_idx[0]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKnUozbTuzJs"
      },
      "source": [
        "In other words: ties are broken in favor of the samples that happen to be recorded earlier in the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2p7malCcuzJs"
      },
      "source": [
        "On a larger scale, that means that some samples will have too much influence - they will appear over and over again as nearest neighbors, *just because they are earlier in the data* - while some samples will not appear as nearest neighbors at all simply because of this tiebreaker behavior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B17dRpwNuzJs"
      },
      "source": [
        "A better tiebreaker behavior would be to randomly sample from neighbors with equal distance. Fortunately, this is an easy fix:\n",
        "\n",
        "-   We had been using `argsort` to get the K smallest distances to each test point. However, if there are more than K training samples that are at the minimum distance for a particular test point (i.e.¬†a tie of more than K values, all having the minimum distance), `argsort` will return the first K of those in order of their index in the distance matrix (their order in `idx_tr`).\n",
        "-   Now, we will use an alternative, `lexsort`, that sorts first by the second argument, then by the first argument; and we will pass a random array as the first argument:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqLBnlUeuzJs"
      },
      "outputs": [],
      "source": [
        "# make a random matrix\n",
        "r_matrix = np.random.random(size=(distances_custom.shape))\n",
        "\n",
        "# sort using lexsort - first sort by distances_custom, then by random matrix in case of tie\n",
        "distances_sorted= np.lexsort((r_matrix, distances_custom))\n",
        "nn_lists = distances_sorted[:, :k]\n",
        "\n",
        "# map indices in distance matrix back to indices in `X` and `y`\n",
        "nn_lists_idx = b_idx[nn_lists]\n",
        "\n",
        "# predict using mode of nns\n",
        "y_pred =  [y.iloc[nn].mode()[0] for nn in nn_lists_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVNjhQpnuzJs"
      },
      "outputs": [],
      "source": [
        "X.iloc[nn_lists_idx[0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qi6deETIuzJs"
      },
      "outputs": [],
      "source": [
        "y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mrElj2HuzJs"
      },
      "source": [
        "Now, if you run the few cells above several times, you will see that the ‚Äútie‚Äù is broken in favor of *different* equally distance samples each time, and the prediction changes accordingly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70VRDaKmuzJs"
      },
      "source": [
        "Let‚Äôs get the accuracy of *this* classifier, with the better tiebreaker behavior, on our entire test set -"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0Etym9VuzJs"
      },
      "outputs": [],
      "source": [
        "distances_custom = custom_distance(X.iloc[idx_ts].to_numpy(), X.iloc[idx_tr].to_numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDTTckTquzJs"
      },
      "outputs": [],
      "source": [
        "# make a random matrix\n",
        "r_matrix = np.random.random(size=(distances_custom.shape))\n",
        "\n",
        "# sort using lexsort - first sort by distances_custom, then by random matrix in case of tie\n",
        "distances_sorted= np.lexsort((r_matrix, distances_custom))\n",
        "nn_lists = distances_sorted[:, :k]\n",
        "\n",
        "# map indices in distance matrix back to indices in `X` and `y`\n",
        "nn_lists_idx = idx_tr[nn_lists]\n",
        "\n",
        "# predict using mode of nns\n",
        "y_pred =  [y.iloc[nn].mode()[0] for nn in nn_lists_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNSUyzjauzJs"
      },
      "outputs": [],
      "source": [
        "accuracy_score(y.iloc[idx_ts], y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "va0w3tMYuzJs"
      },
      "source": [
        "This classifier is less ‚Äúfragile‚Äù - less sensitive to the draw of training data.\n",
        "\n",
        "Depending on the random draw of training and test data, it may or may not have better performance for a particular split - but that‚Äôs largely because our classifier is just not very good. Once we have created a better overall classifier, then having a more reasonable tiebreaker behavior will make a big difference!\n",
        "\n",
        "For the rest of this project, we will use the random tiebreaker behavior whenever we train a KNN classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gsi42eo0uzJs"
      },
      "source": [
        "Now, let‚Äôs talk about the other major problem:\n",
        "\n",
        "-   the distance metric does not tell us how *similar* two samples are, when there are samples with missing values,\n",
        "\n",
        "Consider the following ‚Äúnew‚Äù sample:\n",
        "\n",
        "    [1, 1, 1]\n",
        "\n",
        "and these training samples:\n",
        "\n",
        "    [ [1,   1,   1],\n",
        "      [nan, nan, nan],\n",
        "      [0,   0,   0]]\n",
        "\n",
        "Using our `custom_distance` function, we would compute:\n",
        "\n",
        "-   the distance between the ‚Äúnew‚Äù sample and the first training sample is 0,\n",
        "-   the distance between the ‚Äúnew‚Äù sample and the third training sample is 3, the max possible value\n",
        "\n",
        "all of which makes sense. However, the distance between the ‚Äúnew‚Äù sample and the second training sample - which has all missing values - is also 0!\n",
        "\n",
        "In fact, this sample will be a ‚Äúnearest neighbor‚Äù of *every* test sample! But, it‚Äôs not necessarily *really* similar to those other test samples. We just *don‚Äôt have any information* by which to judge how similar it is to other samples. These values are *unknown*, not *similar*.\n",
        "\n",
        "The case with an all-NaN training sample is a bit extreme, but it illustrates how our simple distance metric is problematic in other situations as well. In general, when there are no missing values, for a pair of samples each feature is either *similar* or *different*. Thus a metric like L1 distance, which explicitly measures the extent to which features are *different*, also implicitly captures the extent to which features are *similar*.\n",
        "\n",
        "When samples can have missing values, though, for a pair of samples each feature is either *similar*, *different*, or *unknown* (one or both samples is missing that value). In this case, a distance metric that only measures the extent of *difference* (like L1 or L2 distance) does not capture whether the features that are not different are *similar* or *unknown*. (Our custom distance metric, which is an L1 distance, treats values that are *unknown* as if they are *similar* - neither one increases the distance.) Similarly, a distance metric that only measures the extent of *similarity* would not capture whether the features that are not similar are *different* or *unknown*.\n",
        "\n",
        "Logically, the distance between a known value and an unknown value (or between two unknown values) is undefined - we do not know how different the values are. However, we cannot classify samples with an ‚Äúundefined‚Äù distance, so our `custom_distance` uses [`np.nansum`](https://numpy.org/doc/stable/reference/generated/numpy.nansum.html) which ignores missing values and does not include them when summing the feature-wise distance. Our distance metric effectively only considers difference, not similarity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZOYoA1xuzJs"
      },
      "source": [
        "A better distance metric will consider the level of disagreement between samples, but will also consider the level of agreement. That will be part of your assignment - to write a new `custom_distance`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qt4qM_UKuzJs"
      },
      "source": [
        "### Use K-fold CV to select the number of neighbors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WL6oh1scuzJs"
      },
      "source": [
        "In the previous example, we set the number of neighbors to 3, rather than letting this value be dictated by the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9xp82CLuzJs"
      },
      "source": [
        "As a next step, to improve the classifier performance, we can use K-fold CV to select the number of neighbors. Note that depending how we do it, this can be *very* computationally expensive, or it can be not much more computationally expensive than just fixing the number of neighbors ourselves.\n",
        "\n",
        "The most expensive part of the algorithm is computing the distance to the training samples. This is $O(nd)$ for each test sample, where $n$ is the number of training samples and $d$ is the number of features. If we can make sure this computation happens only once, instead of once per fold, this process will be fast."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lusZbn-RuzJs"
      },
      "source": [
        "Here, we pre-compute our distance matrix for *every* training sample:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPm5eyOWuzJs"
      },
      "outputs": [],
      "source": [
        "# pre-compute a distance matrix of training vs. training data\n",
        "distances_kfold = custom_distance(X.iloc[idx_tr].to_numpy(), X.iloc[idx_tr].to_numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seOBeC6JuzJs"
      },
      "source": [
        "Now, we‚Äôll use K-fold CV.\n",
        "\n",
        "In each fold, as always, we‚Äôll further divide the training data into validation and training sets.\n",
        "\n",
        "Then, we‚Äôll select the *rows* of the pre-computed distance matrix corresponding to the *validation* data in this fold, and the *columns* of the pre-computed distance matrix corresponding to the *training* data in this fold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tv7ET5G_uzJt"
      },
      "outputs": [],
      "source": [
        "n_fold = 5\n",
        "k_list = np.arange(1, 301, 10)\n",
        "n_k = len(k_list)\n",
        "acc_list = np.zeros((n_k, n_fold))\n",
        "\n",
        "kf = KFold(n_splits=n_fold, shuffle=True)\n",
        "\n",
        "for isplit, idx_k in enumerate(kf.split(idx_tr)):\n",
        "\n",
        "  print(\"Iteration %d\" % isplit)\n",
        "\n",
        "  # Outer loop: select training vs. validation data (out of training data!)\n",
        "  idx_tr_k, idx_val_k = idx_k\n",
        "\n",
        "  # get target variable values for validation data\n",
        "  y_val_kfold = y.iloc[idx_tr[idx_val_k]]\n",
        "\n",
        "  # get distance matrix for validation set vs. training set\n",
        "  distances_val_kfold   = distances_kfold[idx_val_k[:, None], idx_tr_k]\n",
        "\n",
        "  # generate a random matrix for tie breaking\n",
        "  r_matrix = np.random.random(size=(distances_val_kfold.shape))\n",
        "\n",
        "  # loop over the rows of the distance matrix and the random matrix together with zip\n",
        "  # for each pair of rows, return sorted indices from distances_val_kfold\n",
        "  distances_sorted = np.lexsort((r_matrix, distances_val_kfold))\n",
        "\n",
        "  # Inner loop: select value of K, number of neighbors\n",
        "  for idx_k, k in enumerate(k_list):\n",
        "\n",
        "    # now we select the indices of the K smallest, for different values of K\n",
        "    # the indices in  distances_sorted are with respect to distances_val_kfold\n",
        "    # from those - get indices in idx_tr_k, then in X\n",
        "    nn_lists_idx = idx_tr[idx_tr_k[distances_sorted[:,:k]]]\n",
        "\n",
        "    # get validation accuracy for this value of k\n",
        "    y_pred =  [y.iloc[nn].mode()[0] for nn in nn_lists_idx]\n",
        "    acc_list[idx_k, isplit] = accuracy_score(y_val_kfold, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rob4YJguzJt"
      },
      "source": [
        "Here‚Äôs how the validation accuracy changes with number of neighbors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYc49hyCuzJt"
      },
      "outputs": [],
      "source": [
        "plt.errorbar(x=k_list, y=acc_list.mean(axis=1), yerr=acc_list.std(axis=1)/np.sqrt(n_fold-1));\n",
        "\n",
        "plt.xlabel(\"k (number of neighbors)\");\n",
        "plt.ylabel(\"K-fold accuracy\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQ2_f4DuuzJt"
      },
      "source": [
        "Using this, we can find a better choice for k (number of neighbors):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSfLe2LXuzJt"
      },
      "outputs": [],
      "source": [
        "best_k = k_list[np.argmax(acc_list.mean(axis=1))]\n",
        "print(best_k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMihSw8nuzJt"
      },
      "source": [
        "Now, let‚Äôs re-run our KNN algorithm using the entire training set and this `best_k` number of neighbors, and check its accuracy on the test data?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqTSQd8UuzJt"
      },
      "outputs": [],
      "source": [
        "distances_custom = custom_distance(X.iloc[idx_ts].to_numpy(), X.iloc[idx_tr].to_numpy())\n",
        "\n",
        "# make a random matrix\n",
        "r_matrix = np.random.random(size=(distances_custom.shape))\n",
        "\n",
        "# sort using lexsort - first sort by distances_custom, then by random matrix in case of tie\n",
        "distances_sorted= np.lexsort((r_matrix, distances_custom))\n",
        "nn_lists = distances_sorted[:, :best_k]\n",
        "\n",
        "# map indices in distance matrix back to indices in `X` and `y`\n",
        "nn_lists_idx = idx_tr[nn_lists]\n",
        "\n",
        "# predict using mode of nns\n",
        "y_pred =  [y.iloc[nn].mode()[0] for nn in nn_lists_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-q4bNQduzJt"
      },
      "outputs": [],
      "source": [
        "accuracy_score(y.iloc[idx_ts], y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXbYdtKQuzJt"
      },
      "source": [
        "It‚Äôs better - but is it the best we can do?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckKlsiLbuzJt"
      },
      "source": [
        "### Summarizing our basic classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlfJv1w4uzJt"
      },
      "source": [
        "Our basic classifier, with the improvements we introduced:\n",
        "\n",
        "-   uses three features (age, race, and education) to predict a respondent‚Äôs vote\n",
        "-   doesn‚Äôt mind if there are NaNs in the data (unlike the `sklearn` implementation, which throws an error)\n",
        "-   uses a random tiebreaker if there are multiple training samples with the same distance to the test sample\n",
        "-   uses the number of neighbors with the best validation accuracy, according to K-fold CV.\n",
        "\n",
        "But, there are some outstanding issues:\n",
        "\n",
        "-   the distance metric only cares about the degree of disagreement (difference) between two samples, and doesn‚Äôt balance it against the degree of agreement (similarity).\n",
        "-   we have only used three features, out of many more available features.\n",
        "\n",
        "For this assignment, you will create an even better classifier by improving on those two issues."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGFjSu6uuzJt"
      },
      "source": [
        "## Create a better classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3aLFJNGuzJt"
      },
      "source": [
        "In the remaining sections of this notebook, you‚Äôll need to fill in code to:\n",
        "\n",
        "-   implement a custom distance metric\n",
        "-   encode more features\n",
        "-   implement feature weighting\n",
        "-   ‚Äútrain‚Äù and evaluate your final classifier, including K-Fold CV to select the best value for number of neighbors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UEDOl6fuzJt"
      },
      "source": [
        "### Create a better distance metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_MkL2OyuzJt"
      },
      "source": [
        "Your first task is to improve on the basic distance metric we used above. Your distance metric should satisfy the following criteria:\n",
        "\n",
        "1.  if two samples are identical, the distance between them should be zero.\n",
        "2.  as the extent of *difference* between two samples increases, the distance should increase.\n",
        "3.  as the extent of *similarity* between two samples increases, the distance should decrease.\n",
        "4.  if in a pair of samples one or both have a NaN value for a given feature, the similarity or difference of this feature is *unknown*. Your distance metric should compute a smaller distance for a pair of samples with many similarities (even if there is some small difference) than for a pair of samples with mostly unknown similarity.\n",
        "\n",
        "Note that `numpy` includes many functions that are helpful when working with arrays that have NaN values, including mathematical functions like [sum](https://numpy.org/doc/stable/reference/generated/numpy.nansum.html), [product](https://numpy.org/doc/stable/reference/generated/numpy.nanprod.html), [max](https://numpy.org/doc/stable/reference/generated/numpy.nanmax.html) and [min](https://numpy.org/doc/stable/reference/generated/numpy.nanmin.html), and logic functions like [isnan](https://numpy.org/doc/stable/reference/generated/numpy.isnan.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wk8k8UBuzJt"
      },
      "source": [
        "To realize these goals, you should modify the `custom_distance` function as follows:\n",
        "\n",
        "-   for a row in `a` and each row in `b`, count the number of values missing in *either* `a`, `b`, or both.\n",
        "-   then, compute a penalty that scales with ‚Äúnumber of missing values‚Äù. Add this penalty to the L1 distance that the original `custom_distance` computes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-pMIANIuzJt"
      },
      "source": [
        "#### üìù Specific requirements\n",
        "\n",
        "**Function signature**: Your `custom_distance` should accept a 2D array `a` (representing a set of ‚Äúnew‚Äù samples) as its first argument, and a 2D array `b` (representing a set of training samples) as its second argument. Then, it returns an array with shape `(n_rows_a, n_rows_b)`, with the distance between each row of `a` and each row of `b`.\n",
        "\n",
        "**Missing values**: Your `custom_distance` function should *not* impute 0s or any other value in place of `NaN` values in either `a` or `b`.\n",
        "\n",
        "**Intermediate variables**: Your function should compute the following values, using each row in `a` against the rows in `b`:\n",
        "\n",
        "-   `l1` = total magnitude of ‚Äúdisagreements‚Äù/‚Äúknown dissimilarity‚Äù between `a` and each row in `b` (This is the L1 distance for known values). It is already computed by the original `custom_distance`.\n",
        "-   `n_nan` = total number of NaN/‚Äúunknown‚Äù values where either `a` *OR* the corresponding row of `b` (or both!) has a NaN.\n",
        "\n",
        "and you should use both of these (*not* only `l1`) in your distances."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNWSaNo-uzJt"
      },
      "source": [
        "#### Implement your distance metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKxdUgIHuzJt"
      },
      "outputs": [],
      "source": [
        "#grade (write your code in this cell and DO NOT DELETE THIS LINE)\n",
        "\n",
        "# TODO - implement distance metric\n",
        "\n",
        "def custom_distance(a, b):\n",
        "\n",
        "  # fill in your solution here!\n",
        "\n",
        "  # this is just a placeholder - your function shouldn't actually return\n",
        "  # all zeros ;)\n",
        "  distances = np.zeros(a.shape[0], b.shape[0])\n",
        "  return distances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-2f5UJQuzJt"
      },
      "source": [
        "#### Test cases for your distance metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a247-8EauzJt"
      },
      "source": [
        "You can use these test samples to check your work. (But, your metric should also satisfy the criteria in general - not only for these specific cases!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxk5tnE0uzJt"
      },
      "source": [
        "First criteria: if two samples are identical, the distance between them should be zero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXNMELVsuzJt"
      },
      "outputs": [],
      "source": [
        "a = np.array([[0, 1, 0,      1, 0, 0.3]] )  # A0 - test sample\n",
        "b = np.array([[0, 1, 0,      1, 0, 0.3]] )  # B0 - same as A0, should have 0 distance\n",
        "custom_distance(a, b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmHP4gPguzJu"
      },
      "source": [
        "Second criteria: as the extent of *difference* between two samples increases, the distance should increase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaLjoWt9uzJu"
      },
      "source": [
        "These should have *increasing* distance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUd__hjduzJu"
      },
      "outputs": [],
      "source": [
        "a = np.array([[0, 1, 0,      1, 0, 0.3]] )  # A0 - test sample\n",
        "b = np.array([[0, 1, 0,      1, 0, 0.3],              # B0 - same as A0, should have 0 distance\n",
        "              [0, 1, 0,      1, 0, 0.5],              # B1 - has one small difference, should have larger distance than B0\n",
        "              [0, 1, 0,      1, 0, 1  ],              # B2 - has more difference, should have larger distance than B1\n",
        "              [0, 0, 0,      1, 0, 0  ],              # B3 - has even more difference\n",
        "              [1, 0, 1,      0, 1, 0  ]])             # B4 - has the most difference\n",
        "custom_distance(a, b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzfBIp5euzJu"
      },
      "source": [
        "These should have *decreasing* distance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oVtOehRuzJu"
      },
      "outputs": [],
      "source": [
        "a = np.array([[0, 1, 0, 1, 0, 1]] )            # A0 - test sample\n",
        "b = np.array([[1, 0, 1, 0, 1, 0],              # B0 - completely different, should have large distance\n",
        "              [1, 0, 1, 0, 1, np.nan],         # B1 - less difference than B0, should have less distance\n",
        "              [1, 0, 1, 0, np.nan, np.nan]])   # B2 - even less difference than B1, should have less distance\n",
        "custom_distance(a, b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBhjG87BuzJu"
      },
      "source": [
        "Third criteria: as the extent of *similarity* between two samples increases, the distance should decrease."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COGumkNbuzJu"
      },
      "source": [
        "These should have *increasing* distance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHqJp2_buzJu"
      },
      "outputs": [],
      "source": [
        "a = np.array([[0, 1, 0, 1, 0, 0.3]] )  # A0 - test sample\n",
        "b = np.array([[0, 1, 0, 1, 0, 0.3],              # B0 - same as A0, should have 0 distance\n",
        "              [0, 1, 0, 1, 0, np.nan],           # B1 - has less similarity than B0, should have larger distance\n",
        "              [0, 1, 0, 1, np.nan, np.nan],      # B2 - has even less similarity, should have larger distance\n",
        "              [0, np.nan, np.nan, np.nan, np.nan, np.nan]])     # B3 - has least similarity, should have larger distance\n",
        "custom_distance(a, b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVpfLr5IuzJu"
      },
      "source": [
        "Fourth criteria: if in a pair of samples one or both have a NaN value for a given feature, the similarity or difference of this feature is *unknown*. Your distance metric should compute a smaller distance for a pair of samples with many similarities (even if there is some small difference) than for a pair of samples with mostly unknown similarity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiUdfi9LuzJu"
      },
      "source": [
        "These should have *increasing* distance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChkhSzdpuzJu"
      },
      "outputs": [],
      "source": [
        "a = np.array([[0, np.nan, 0, 1, np.nan, 0.3]] )  # A0 - test sample\n",
        "b = np.array([[0, np.nan, 0, 1, 0,      0.5],                # B0 - three similar features, one small difference\n",
        "              [0, np.nan, np.nan, np.nan, np.nan, np.nan]])  # B1 - much less similarity than B0, should have larger distance\n",
        "\n",
        "custom_distance(a, b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCmuoKmJuzJu"
      },
      "source": [
        "### Encode more features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXOAzlVFuzJu"
      },
      "source": [
        "Our basic classifier used three features: age, race, and education. But there are many more features in this data that may be predictive of vote:\n",
        "\n",
        "-   More demographic information: `age10`, `sex`, `latino`, `income20`, `newvoter`, `work`, `unionhh1`, `married`, `child12`, `vetvoter`, `brnagain`, `relign18`, `lgbt`\n",
        "-   Opinions about political issues and about what factors are most important in determining which candidate to vote for: `issue20`, `qlt20`, `issvqual`, `favblm`, `acasupre`, `fairjust`, `facemask`, `nec`, `govtangr`, `econvcor`, `life`, `containc`, `covidhar`, `racism20`, `countacc`, `voteexpe`, `abortion`, `finsit`, `supreme1`, `climatec`\n",
        "\n",
        "in addition to the features `age`, `qraceai`, and `educ18`.\n",
        "\n",
        "You will try to improve the model by adding some of these features.\n",
        "\n",
        "Note: you may not add features that are **not** included in the lists above. We are deliberately excluding features that ask about the candidates, or about political affiliations, because we are trying to develop a model that predicts vote using *only* demographic information and opinions about issues."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBP25hLouzJu"
      },
      "source": [
        "Refer to the PDF documentation to see the question and the possible answers corresponding to each of these features. You may also choose to do some exploratory data analysis, to help you understand these features better.\n",
        "\n",
        "For your convenience, here are all the possible answers to those survey questions (you will need the text of the possible answers in order to design mappings for ordinal features):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqTNnUWMuzJu"
      },
      "outputs": [],
      "source": [
        "features = ['issue20', 'qlt20', 'issvqual', 'favblm', 'acasupre', 'fairjust', 'facemask', 'nec', 'govtangr', 'econvcor',  'life', 'containc', 'covidhar', 'racism20', 'countacc', 'voteexpe', 'abortion', 'finsit', 'supreme1', 'climatec',\n",
        "'age10', 'sex', 'latino', 'income20', 'newvoter', 'work', 'unionhh1', 'married', 'child12', 'vetvoter', 'brnagain', 'relign18', 'lgbt', 'age', 'educ18', 'qraceai']\n",
        "\n",
        "for f in features:\n",
        "  print(f)\n",
        "  print(df[f].value_counts())\n",
        "  print(\"***************************************************\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zr1JKaH4uzJu"
      },
      "source": [
        "#### üìù Specific requirements\n",
        "\n",
        "From among the features listed above, you should encode all of the features that seem *useful* according to the anaylsis we are about to do, although it is up to you to decide where the (qualitative) threshold for ‚Äúuseful‚Äù lies. (We will apply feature weighting later, so it should not be very harmful to include not-so-relevant features; you can keep this in mind when deciding what is ‚Äúuseful‚Äù.)\n",
        "\n",
        "However, even if you don‚Äôt find this many useful features, you must encode at least **eight** features, including:\n",
        "\n",
        "-   at least four features that are encoded using an ordinal encoder because they have a logical order (and you should include an explicit mapping for these), and\n",
        "-   at least four features that are encoded using one-hot encoding because they have no logical order. (Note: count four features *before* encoding, not four columns *after* encoding.)\n",
        "\n",
        "Binary features - features that can take on only two values - ‚Äúcount‚Äù toward either category.\n",
        "\n",
        "(If you decide to use the features I used above, they do ‚Äúcount‚Äù as part of the four. For example, you could use age, education, and two additional ordinal-encoded features, and race and three other one-hot-encoded features.)\n",
        "\n",
        "Furthermore, for full credit, you *must* have features that seem *useful* for each survey version."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2s4H7abQuzJu"
      },
      "source": [
        "First, you will want to make sure that you include features that are available in many samples. For each of the features listed above, count the number of non-missing values in the training data. (You can write one line of code using `pandas` functions!)\n",
        "\n",
        "The result should be a `pandas` DataSeries, where the index is the feature name and the value is the number of non-missing values in the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOfC8KbUuzJu"
      },
      "outputs": [],
      "source": [
        "# TODO - count number of non-missing values per feature\n",
        "# non_nan = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TR5RfP6wuzJu"
      },
      "source": [
        "Next, you will want to make sure that you are including features that are available on each of the survey versions - it would be a bad idea (for example) to pick mostly features that are only on survey version 4, but not on 1, 2, and 3! Add a `groupby` to the line of code you wrote above, to group by survey version.\n",
        "\n",
        "The result should be a `pandas` DataFrame, where the index is the version and there is a column for each feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIV5VB-vuzJu"
      },
      "outputs": [],
      "source": [
        "# TODO - count number of non-missing values per feature, grouped by survey version\n",
        "# non_nan_version = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9LteaUYuzJv"
      },
      "source": [
        "If you have computed it correctly, the following line should visualize the results -"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAgztLQIuzJv"
      },
      "outputs": [],
      "source": [
        "# TODO - visualize results\n",
        "\n",
        "df_melted = non_nan_version.reset_index().melt(id_vars='version', var_name=\"Column\", value_name=\"Count\")\n",
        "g = sns.FacetGrid(df_melted, col=\"Column\", margin_titles=True, height=3, aspect=1, sharey=True, col_wrap = 5)\n",
        "g.map_dataframe(sns.barplot, x=\"version\", y=\"Count\")\n",
        "g.set_axis_labels(\"Survey Version\", \"Non-NaN Count\")\n",
        "g.set_titles(\"{col_name}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIdvtKhGuzJv"
      },
      "source": [
        "**TODO**: Comment on the results. Which features are available in multiple survey versions? Which features are only in some survey versions?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4FoA1jvuzJv"
      },
      "source": [
        "Of course, you want to make sure you select features from each survey version - but you should also make sure to select features that are useful for the prediction task.\n",
        "\n",
        "For the features that *are* available on all survey versions, create a figure similar to Figure 6 in the ‚ÄúFeature selection‚Äù handout, where for each value of the feature, you show the proportion of samples in each class. You should have one plot, with a subplot for each feature that is available on all survey versions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXrgYfSiuzJv"
      },
      "source": [
        "#### üìù Specific requirements\n",
        "\n",
        "As per the course policy described in the syllabus, you are welcome to use generate AI specifically to help you generate `matplotlib` or `seaborn` code for data visualization, although of course you are responsible for the final plot. Your grade will be determined by both correctness, and effectiveness of the visualization in showing the relevant trends.\n",
        "\n",
        "If you use AI to generate plotting code for this assignment, you must add a text cell to this notebook in which you (1) describe the AI assistant you used (e.g.¬†ChatGPT, Gemini), (2) quote the exact prompt or sequence of prompts you used, and (3) describe changes you made to the AI-generated plotting code, and why you made them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stWckG6kuzJv"
      },
      "outputs": [],
      "source": [
        "# TODO - plot the proportion of samples in each class, for each value of the feature,\n",
        "# for each of the features that are on *all* survey versions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__6mR_PAuzJv"
      },
      "source": [
        "**TODO**: Comment on the results. Among the features that are available on all survey versions, which seem most useful for the prediction task? Which seem least useful to the prediction task? Explain how the plots you created support your conclusions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAhxjUZTuzJv"
      },
      "source": [
        "Now, make a similar plot for features on survey Version 1 only:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPDgJuuVuzJv"
      },
      "outputs": [],
      "source": [
        "# TODO - plot the proportion of samples in each class, for each value of the feature,\n",
        "# for each of the features that are only on survey Version 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PM2S0K1uzJv"
      },
      "source": [
        "for features on survey Version 2 only:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxPbxclluzJv"
      },
      "outputs": [],
      "source": [
        "# TODO - plot the proportion of samples in each class, for each value of the feature,\n",
        "# for each of the features that are only on survey Version 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sR5qM3suzJv"
      },
      "source": [
        "for features on survey Version 3 only:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtjlFCnsuzJv"
      },
      "outputs": [],
      "source": [
        "# TODO - plot the proportion of samples in each class, for each value of the feature,\n",
        "# for each of the features that are only on survey Version 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl5uTujpuzJv"
      },
      "source": [
        "and for features on survey Version 4 only:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kq7FvB7tuzJv"
      },
      "outputs": [],
      "source": [
        "# TODO - plot the proportion of samples in each class, for each value of the feature,\n",
        "# for each of the features that are only on survey Version 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IwbVoMRuzJv"
      },
      "source": [
        "**TODO**: Comment on the results. For each survey version, describe the features that seem most useful and least useful for the prediction task. Explain how the plots you created support your conclusions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUzJr9CMuzJv"
      },
      "source": [
        "**TODO**: Now that you have a better idea of which features are useful for each survey version: Which features will you include in your model, and why (with reference to the figures you created)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAhF9Xn9uzJv"
      },
      "source": [
        "#### Encode ordinal features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsQpr5GPuzJv"
      },
      "source": [
        "In the following cells, prepare your ordinal encoded features as demonstrated in the ‚ÄúPrepare data \\> Encode ordinal features‚Äù section earlier in this notebook.\n",
        "\n",
        "Use at least four features that are encoded using an ordinal encoder. (You can choose which features to include, but they should be either binary features, or features for which the values have a logical ordering that should be preserved in the distance computations!)\n",
        "\n",
        "Also:\n",
        "\n",
        "-   Save the ordinal-encoded columnns in a data frame called `df_enc_ord`.\n",
        "-   You should explicitly specify the mappings for these, so that you can be sure that they are encoded using the correct logical order.\n",
        "-   For some questions, there is also an ‚ÄúOmit‚Äù answer - if a respondent left that question blank on the questionnaire, the value for that question will be ‚ÄúOmit‚Äù. Since ‚ÄúOmit‚Äù has no logical place in the order, we‚Äôre going to treat these as missing values: don‚Äôt include ‚ÄúOmit‚Äù in your `mapping_ord` dictionary, and then these Omit values will be encoded as NaN.\n",
        "-   Make sure to scale each column to the range 0-1, as demonstrated in the ‚ÄúPrepare data \\> Encode ordinal features‚Äù section earlier in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hh1vsDoluzJv"
      },
      "outputs": [],
      "source": [
        "#grade (write your code in this cell and DO NOT DELETE THIS LINE)\n",
        "\n",
        "# TODO - encode ordinal features\n",
        "\n",
        "# set up mapping dictionary and list of features to encode with ordinal encoding\n",
        "\n",
        "# use map to get the encoded columns, save in df_enc_ord\n",
        "df_enc_ord = ...\n",
        "\n",
        "# scale each column to the range 0-1\n",
        "df_enc_ord =\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4gnPt7TuzJv"
      },
      "source": [
        "Look at the encoded data to check your work:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6Wv_SjguzJv"
      },
      "outputs": [],
      "source": [
        "df_enc_ord.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXVUgvuNuzJv"
      },
      "source": [
        "#### Encode categorical features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQhCl6PHuzJv"
      },
      "source": [
        "In the following cells, prepare your categorical encoded features as demonstrated in the ‚ÄúPrepare data \\> Encode categorical features‚Äù section earlier in this notebook.\n",
        "\n",
        "Use at least four features that are encoded using an categorical encoder. (You can choose which features to include, but they should be either binary features, or features for which the values do *not* have a logical ordering that should be preserved in the distance computations!)\n",
        "\n",
        "Also:\n",
        "\n",
        "-   Save the categorical-encoded columnns in a data frame called `df_enc_oh`.\n",
        "-   As in the example given in the ‚ÄúPrepare data \\> Encode categorical features‚Äù section of this notebook, use the `get_dummies` function and specify that the data type should be `np.int32`.\n",
        "-   Use the name of the feature as the prefix for the new column, with the exception of the `qraceai` feature for which you should use the prefix `race` (like in the example).\n",
        "-   Make sure to preserve NaNs - if a sample did not have a value for a particular feature, assign a NaN to all columns corresponding to that feature for that sample.\n",
        "-   For some questions, there is also an ‚ÄúOmit‚Äù answer - if a respondent left that question blank on the questionnaire, the value for that question will be ‚ÄúOmit‚Äù. We‚Äôre going to treat these as missing values. Before encoding the values, you should drop the column corresponding to the ‚ÄúOmit‚Äù value from the data frame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fwfKeoguzJv"
      },
      "outputs": [],
      "source": [
        "#grade (write your code in this cell and DO NOT DELETE THIS LINE)\n",
        "\n",
        "# TODO - encode categorical features\n",
        "\n",
        "# use get_dummies as shown above to get the encoded columns, stack and save in df_enc_oh\n",
        "df_enc_oh = ...\n",
        "\n",
        "# drop the Omit columns, if any of these are in the data frame\n",
        "# df_enc_oh.drop([... list all the \"Omit\" columns here to drop them ...],\n",
        "#                axis=1, inplace=True, errors='ignore')\n",
        "\n",
        "# if a respondent did not answer a question, make sure they have NaN in all the columns corresonding to that question\n",
        "# (using our example above)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7v1ZzKtuzJv"
      },
      "source": [
        "#### Stack columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3V7BgIhuzJv"
      },
      "source": [
        "Now, we‚Äôll create a combined data frame with all of the encoded features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92K9pacIuzJv"
      },
      "outputs": [],
      "source": [
        "#grade (write your code in this cell and DO NOT DELETE THIS LINE)\n",
        "\n",
        "X = pd.concat([df_enc_oh, df_enc_ord], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sTNi61KuzJw"
      },
      "outputs": [],
      "source": [
        "X.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agaoLeyPuzJw"
      },
      "source": [
        "### Feature weighting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljnB4BqcuzJw"
      },
      "source": [
        "Because the K nearest neighbor classifier weights each feature equally in the distance metric, including features that are not relevant for predicting the target variable can actually make performance worse.\n",
        "\n",
        "To improve performance, we will use feature weights, so that more important features are scaled up (and count more when computing distance between samples) and less important features are scaled down (and count less, when computing distance).\n",
        "\n",
        "Currently, every feature is on a scale from 0 to 1. We will compute feature weights, then multiply each column by its corresponding weight, so that each feature is scaled from 0 to ‚Äúfeature weight‚Äù."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRrKtkPUuzJw"
      },
      "source": [
        "#### üìù Specific requirements\n",
        "\n",
        "There are many options for feature selection or feature weighting. For this assignment, you will use a naive search strategy (score each feature independently) using Weight of Evidence (WoE) and Information Value (IV) as the scoring function.\n",
        "\n",
        "WoE and IV may be used in\n",
        "\n",
        "-   binary classification problems\n",
        "-   with categorical features (each feature takes on a set of discrete values) (or, continuous features that are ‚Äúbinned‚Äù)\n",
        "\n",
        "to quantify how well a feature distinguishes between the 0 and 1 classes. Since this voter classification task is a binary classification problem with categorical features, we can use this approach.\n",
        "\n",
        "For each possible value $j$ of a feature $X$, WoE is computed as:\n",
        "\n",
        "$$\\text{WoE}_j = \\ln \\left( \\frac{P(X = x_j \\mid y = 0)}{P(X = x_j \\mid y = 1)} \\right)$$\n",
        "\n",
        "Here, the numerator is the proportion of samples in the ‚Äúnegative‚Äù (`0`) class for which the value of the featue is $x_j$, and the denominator is the proportion of samples in the ‚Äúpositive‚Äù (`1`) class for which the value of the featue is $x_j$.\n",
        "\n",
        "A strongly negative WoE means that value of a feature is strongly associated with belonging to the positive class; a strongly positive WoE means that value of a feature is strongly associated with belonging to the negative class.\n",
        "\n",
        "The `pandas` built-in `groupby` and `agg` functions (which we practiced in the Week 1 Colab lesson) will be useful for computing the WoE of each feature.\n",
        "\n",
        "At this point, you will have a WoE for each possible *value* of a feature. Then, the Information Value (IV) summarizes the overall predictive power of a feature by combining the WoE and the difference in class distributions across all possible values of the feature:\n",
        "\n",
        "$$\\text{IV} = \\sum_{j} \\left( P(X = x_j \\mid y = 0) - P(X = x_j \\mid y = 1) \\right) \\cdot \\text{WoE}_j$$\n",
        "\n",
        "Note that you will compute these values *excluding the samples where the value of the feature is missing*. That is, when computing the WoE values and the IV value for a feature, you will first get the subset of the data for which that feature is not missing; then you will compute WoE and IV for that subset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lm39onK_uzJw"
      },
      "source": [
        "In the following cell, implement the `compute_feature_iv` function using the template that is provided.\n",
        "\n",
        "-   The function should return a single scalar IV value for a feature, computed using the rows of X and y for which the feature is *not* missing.\n",
        "-   Use `pandas` and `numpy` functions to compute the IV.\n",
        "-   Use a small `epsilon` constant (`1e-15`) to prevent taking log of zero or dividing by zero. Specifically,\n",
        "-   When computing $P(X = x_j \\mid y = 0)$ and $P(X = x_j \\mid y = 1)$, add `epsilon` to the denominator (to avoid division by zero).\n",
        "-   When computing $\\text{WoE}_j = \\ln \\left( \\frac{P(X = x_j \\mid y = 0)}{P(X = x_j \\mid y = 1)} \\right)$, add `epsilon` to the denominator (to avoid division by zero) and also to the numerator (to avoid taking the log of zero)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pv_w0rWZuzJw"
      },
      "outputs": [],
      "source": [
        "#grade (write your code in this cell and DO NOT DELETE THIS LINE)\n",
        "\n",
        "# TODO - compute IV for a feature\n",
        "\n",
        "def compute_feature_iv(X, y, feature, epsilon=1e-15):\n",
        "\n",
        "    # step 1: get the rows of X and y for which feature is not missing\n",
        "\n",
        "    # step 2: compute the WoE for each value of feature\n",
        "\n",
        "    # step 3: compute the IV for the feature\n",
        "\n",
        "    # return the IV (instead of zero; the zero is just a placeholder)\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KBoAly8uzJw"
      },
      "source": [
        "Next, you will use your `compute_feature_iv` to fill in the array `feat_wt`. Make sure to *only* pass the training data (not test data!) to `compute_feature_iv`.\n",
        "\n",
        "To get your final weighted data set `X_trans`, you will simply multiply the data by `feat_wt`. (The $i$th column in the data is multiplied by the $i$th entry in `feat_wt`.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMZdUiyPuzJw"
      },
      "outputs": [],
      "source": [
        "#grade (write your code in this cell and DO NOT DELETE THIS LINE)\n",
        "\n",
        "# TODO - fill in feat_wt with IV of each feature in your data\n",
        "\n",
        "feat_wt = np.zeros(X.shape[1])\n",
        "# call compute_feature_iv to fill in feat_wt\n",
        "\n",
        "X_trans = X.multiply(feat_wt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZuxNLexuzJw"
      },
      "source": [
        "Check your work:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCpX7KInuzJw"
      },
      "outputs": [],
      "source": [
        "X_trans.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XcL3ToQuzJw"
      },
      "source": [
        "**TODO**: Comment on the results. Which features are assigned the highest weights by your scoring function? Does this make sense, based on what you saw in your data visualization earlier?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SvylpI3uzJw"
      },
      "source": [
        "### Evaluate final classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWn-IKNuuzJw"
      },
      "source": [
        "Finally, you‚Äôll repeat the process (illustrated earlier) of finding the best number of neighbors using K-fold CV, with your ‚Äútransformed‚Äù data (`X_trans`) and your new custom distance metric.\n",
        "\n",
        "Then, you‚Äôll evaluate the performance of your model on the *test* data, using that optimal number of neighbors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LWk7kimuzJw"
      },
      "outputs": [],
      "source": [
        "#grade (write your code in this cell and DO NOT DELETE THIS LINE)\n",
        "\n",
        "# TODO - evaluate - pre-compute distance matrix of training vs. training data\n",
        "\n",
        "distances_kfold = ...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2qTQFM6uzJw"
      },
      "outputs": [],
      "source": [
        "#grade (write your code in this cell and DO NOT DELETE THIS LINE)\n",
        "\n",
        "# TODO - evaluate - use K-fold CV, fill in acc_list\n",
        "\n",
        "n_fold = 5\n",
        "k_list = np.arange(1, 301, 10)\n",
        "n_k = len(k_list)\n",
        "acc_list = np.zeros((n_k, n_fold))\n",
        "\n",
        "# use this random state so your results will match the auto-graders'\n",
        "kf = KFold(n_splits=n_fold, shuffle=True, random_state=3)\n",
        "\n",
        "for isplit, idx_k in enumerate(kf.split(idx_tr)):\n",
        "\n",
        "  # Outer loop\n",
        "\n",
        "  for idx_k, k in enumerate(k_list):\n",
        "\n",
        "    # Inner loop\n",
        "\n",
        "    acc_list[idx_k, isplit] = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mi7AX_EYuzJw"
      },
      "source": [
        "See how the validation accuracy changes with number of neighbors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WScp-_WuuzJw"
      },
      "outputs": [],
      "source": [
        "plt.errorbar(x=k_list, y=acc_list.mean(axis=1), yerr=acc_list.std(axis=1)/np.sqrt(n_fold-1));\n",
        "\n",
        "plt.xlabel(\"k (number of neighbors)\");\n",
        "plt.ylabel(\"K-fold accuracy\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vps7BFhBuzJw"
      },
      "source": [
        "Find the best choice for k (number of neighbors) using the ‚Äúhighest validation accuracy‚Äù rule:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvwK2f2puzJw"
      },
      "outputs": [],
      "source": [
        "#grade (write your code in this cell and DO NOT DELETE THIS LINE)\n",
        "\n",
        "# TODO - evaluate - find best k\n",
        "best_k = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wL2aJ8WYuzJw"
      },
      "source": [
        "Finally, re-run our KNN algorithm using the entire training set and this `best_k` number of neighbors (and your custom distance function, the feature weights as applied to `X_trans`, the random tiebreaker rule, etc.). Check its accuracy on the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CX0VHjSuzJw"
      },
      "outputs": [],
      "source": [
        "#grade (write your code in this cell and DO NOT DELETE THIS LINE)\n",
        "\n",
        "# TODO - evaluate - find accuracy\n",
        "# compute distance matrix for test vs. training data\n",
        "# use KNN with best_k to find y_pred for test data\n",
        "y_pred = ...\n",
        "# compute accuracy\n",
        "acc = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rw3PN6puzJw"
      },
      "outputs": [],
      "source": [
        "print(acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNA7OoCguzJw"
      },
      "source": [
        "Also compute the test accuracy *for each survey version separately*. (Use the `y_pred` you already computed.) Save the results in an array named `acc_by_version`.\n",
        "\n",
        "For full credit, you should achieve no less than 70% test accuracy on any survey version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3OR_U3QuzJw"
      },
      "outputs": [],
      "source": [
        "#grade (write your code in this cell and DO NOT DELETE THIS LINE)\n",
        "\n",
        "# TODO - evaluate - find accuracy grouped by survey version\n",
        "# acc_by_version = np.array([acc_v1, acc_v2, acc_v3, acc_v4])\n",
        "acc_by_version = ..."
      ]
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "colab": {
      "provenance": []
    }
  }
}